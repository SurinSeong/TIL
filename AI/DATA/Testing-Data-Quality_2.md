# Feature Engineering의 중요성

## Feature Engineering

- 데이터의 Feature를 **분석에 적합한 Feature로 변환**하거나 **새로운 Feature를 생성**하는 과정
    
    ![Image](https://github.com/user-attachments/assets/5d316a08-40a1-4b09-82df-2b782687549c)
    
    ![Image](https://github.com/user-attachments/assets/9b789830-99a6-4135-a511-7d1a96e06cdc)
    

### 필요성

- 최근 많은 분야의 데이터들은 변수의 수가 매우 많은 **고차원 데이터**의 특징을 가지고 있음.
- **차원의 저주 (Curse of Dimensionality)** : 고차원 공간에서 데이터가 희소해지고 **거리 개념이 왜곡**되어 알고리즘 성능이 저하되는 현상
- 동등한 설명력을 갖기 위해서는 변수가 증가할 때, 필요한 개체의 수는 기하급수적으로 증가함.
- 실제 데이터의 차원은 매우 크더라도, **내제된 차원은 원래의 차원 수보다 낮은 경우가 대부분**인 것을 이용한다.
    
    ![Image](https://github.com/user-attachments/assets/cdcdea85-eeac-4ae7-8ad7-2d9acf3519e5)
    
    ![Image](https://github.com/user-attachments/assets/50ca2b6b-a208-4fc5-94bc-8bfba0ea5981)
    

### 중요성

![Image](https://github.com/user-attachments/assets/c2e10d50-63be-4765-811f-6b05e8c339fe)

### 기법

1. **Feature 생성**
    - 도메인 지식과 창의성을 바탕으로 **기존 Feature들을 재조합**하여 **새로운 Feature를 만드는 과정**
    - 모델이 더 잘 학습할 수 있도록 **데이터의 유용한 정보를 추가로 제공**한다.
    1. **기초적인 수학적 변환** (사칙연산)
    2. 특정 문제에 대한 **도메인 지식 사용** (예: 주택 가격 예측에서 면적당 가격 = 총 가격 / 면적)
    3. 그룹별 **통계값** (**평균, 최댓값, 최솟값**) 계산 (예: 나이대별 구매 패턴 분석에서 평균 구매 금액 생성)
    4. 텍스트 데이터에서 **문서 길이, 특정 단어 출현 빈도 (TF-IDF), 감정 점수 (Sentiment Score)** 등 새로운 Feature 추출
2. **Feature 변환**
    - 기존 Feature의 값을 **수학적으로 변환**하여 데이터의 특성을 변경
    - 데이터를 **모델이 더 잘 이해할 수 있는 형태로 변환**하는 과정
    1. **스케일링 (Scaling)** : 데이터의 범위를 조정 - **표준화 (Standardization), 정규화 (Normalization)**
    2. **로그 변환 (Log Transformation)** : 값의 분포가 **왜곡된 경우를 완화**
    3. **비닝 (Binning)** : 연속적 데이터를 구간 (bin)으로 나누어 **범주형 데이터로 변환** (예: 나이를 “유아”, “청소년”, “성인”으로 구분)
3. Feature Encoding
    - **범주형 데이터**를 모델이 처리할 수 있는 **수치형 데이터로 변환**하는 과정
    - 모델은 범주형 데이터를 직접 처리하지 못하기 때문에, 인코딩하여 모델 입력에 적합하게 만드는 것이 필수적이다.
    
    | 종류 | 설명 | 예시/방법 |
    | --- | --- | --- |
    | Label Encoding | - **범주형 데이터**를 **고유한 정수값**으로 변환
    - 범주형 데이터가 **순서(ordinal)를 가질 때** 유용
    - 데이터에 순서가 없지만, 모델이 연속적이고 순서적인 관계로 해석하는 위험 | `[”빨강”, “파랑”, “초록”]
    → [0, 1, 2]` |
    | One-Hot Encoding | - 각 범주를 **이진 벡터**로 표현
    - 범주 간 순서나 거리의 왜곡을 방지
    - 범주의 개수가 많아지면 **고차원 데이터 문제** 발생 | `[”빨강”, “파랑”, “초록”]
    → [[1, 0, 0], [0, 1, 0], [0, 0, 1]]` |
    | Target Encoding | - 각 범주를 해당 범주가 가진 **타겟값의 평균**으로 변환
    - 고차원 데이터 문제를 완화하며, 범주 수가 많은 데이터에 적합
    - **타겟 누출 위험**이 있음 | 범주 “빨강”의 타겟 값 평균 = 0.5
    → **“빨강”은 0.5로 변환** |

**※ Feature 생성 및 변환 후, 원래 Feature 삭제 여부를 결정하는 기준**

- 삭제가 적절한 경우
    - 변환된 Feature가 **정보를 완전히 대체**
    - 고차원 데이터에서 **차원을 줄이고 싶은** 경우
    - 원래 Feature가 **노이즈로 작용**하는 경우
- 삭제하지 않는 것이 적절한 경우
    - **모델 성능 비교가 필요**할 경우
    - 원래 Feature의 **해석 가능성이 중요**한 경우
    - **다양한 모델에서 테스트할 계획**이 있는 경우
1. **Feature 선택**
    1. **전역 탐색법 (Exhaustive Search)**
        - 가능한 **모든 경우의 조합**에 대해 모델을 구축한 뒤, **최적의 Feature 조합**을 찾는 방식
        - **변수 선택을 위한 모델 평가 기준** : 회귀 및 분류에서 사용하는 **성능 평가 지표**를 주로 사용한다.
            - 예) 선형회귀분석 - AIC, BIC, adjusted R-squared
        - 하지만, **탐색 소요 시간이 오래 걸려 유효한 방법이 아니다.**
    2. **전진 선택법 (Forward Selection)**
        - 설명 변수가 하나도 없는 모델에서부터 시작해서 **가장 유의미한 변수를 하나씩 추가**해 나가는 방법
        - 한번 **선택된 변수는 제거되지 않는다**.
    3. **후진 소거법 (Backward Elimination)**
        - 모든 변수를 사용해서 구축한 모델에서 **유의미하지 않은 변수를 하나씩 제거**해 나가는 방법
        - **한번 제거된 변수는 다시 선택될 가능성이 없다**.
    4. 단계적 선택법 (Stepwise Selection)
        - **전진 선택법과 후진 소거법을 번갈아**가면서 수행하는 변수 선택 기법
        - 한번 선택된 변수가 이후 과정에서 제거되거나, 제거된 변수가 이후 과정에서 **재선택**될 수 있다.
    5. 유전 알고리즘 (Genetic Algorithm, GA)
        - 전진 선택, 후진 소거, 단계적 선택은 전역 탐색에 비해 매우 효율적이지만, 최적 변수 집합을 찾을 가능성은 낮아진다.
        - 기존 휴리스틱 기법들 보다, **더 많은 시간을 사용해 최적 변수 집합을 찾을 가능성을 높이는 것**이 목표!
        - **진화론적 개념**에서 영감을 받아 최적화를 수행하는 **메타 휴리스틱 알고리즘**
            - 메타 휴리스틱 기법이란, 닫힌 해가 존재하지 않는 복잡한 문제에 대해서 **시행착오를 줄이는 효율적인 해 탐색 기법**이다.
        
        ① 핵심단계
        
        - **선택** : 현재 가능 해집합에서 우수한 해들을 선택하여 다음 세대를 생성하기 위한 부모 세대로 지정
        - **교배** : 선택된 부모 세대들의 유전자를 교환하여 새로운 세대를 생성
        - **돌연변이** : 낮은 확률로 변이를 발생시켜 Local Optimum에서 탈출할 수 있는 기회 제공
        - **적합도** : 각 염색체의 품질을 평가하는 함수
            
            ![Image](https://github.com/user-attachments/assets/33ab3067-4a5d-47f0-8c26-0cd23fd5cc58)
            
        
        ② 유전 알고리즘 절차
        
        ![Image](https://github.com/user-attachments/assets/b387db13-0421-47de-b1f6-bbf84a2b48e7)
        
2. Feature (차원) 축소
    1. **주성분 분석 (PCA, Principal Component Analysis)**
        
        ① PCA **정의 및 목적**
        
        - 고차원 데이터를 저차원으로 축소하면서 **데이터의 분산(정보)를 최대한 보존**하는 선형 차원 축소 기법
        - 이는 데이터를 새로운 좌표계로 변환하여 **데이터의 주요 패턴**을 찾고, **불필요한 차원을 제거하는 것에 사용**한다.
            - 예) 3차원의 데이터를 2차원의 주성분 공간으로 사영시키면 원래 데이터가 가지고 있는 특징의 대부분이 보존된다.
        
        ![Image](https://github.com/user-attachments/assets/18d939d2-cba4-4f25-bb3a-cbad62c995a5)
        
        ② - 1. PCA의 **원리** - **주성분**
        
        ![Image](https://github.com/user-attachments/assets/74ffeb62-171f-45dc-88a4-bd75e8b5819d)
        
        - 데이터의 **분산이 가장 큰 방향**을 나타내는 새로운 축
        - **PC1** : 데이터 분산이 가장 크게 설명하는 축
        - **PC2** : PC1에 직교하면서 분산이 그 다음으로 큰 방향
        
        ② - 2. PCA의 **원리** - **공분산 행렬**
        
        ![Image](https://github.com/user-attachments/assets/d72dd127-597a-47f4-9358-e4534aae5c25)
        
        - 데이터의 변수들 간의 관계를 나타내는 **공분산 행렬**
        - 공분산의 행렬의 고유값과 고유벡터를 사용해 주성분을 정의함
        - 고유값이 큰 순서대로 고유벡터를 정렬하면 중요한 주성분을 구할 수 있음
        - **고유값** : 주성분이 설명하는 분산의 크기
        - **고유벡터** : 주성분의 방향
        
        ③ PCA의 **과정**
        
        ![Image](https://github.com/user-attachments/assets/22e7f068-dd8a-423f-8991-11fdb80ee8db)
        
    2. **t-SNE**
        
        ① t-SNE **정의**
        
        - 고차원 공간에서 가까운 것은 저차원에서도 가깝게, 고차원에서 먼 것은 저차원에서도 멀게 유지하는 것
        - 즉, 고차원 데이터에서 **데이터 포인트 값의 관계를 유지하면서 저차원으로 변환**
        - **비선형 차원 축소 기법**으로, 비선형 변환을 사용하여 복잡한 데이터 구조를 효과적으로 표현
        - 데이터 값의 유사성에 중점을 두며, 글로벌 구조 보다는 **로컬 구조를 더 잘 반영**함.
        
        ![Image](https://github.com/user-attachments/assets/85a30332-edaf-4e20-8f20-d1d640170e36)
        
        ② t-SNE **작동 원리 및 특징**
        
        ![Image](https://github.com/user-attachments/assets/391db2ca-730b-4507-9835-d19201073050)
        
        - **로컬 구조 보존** : **가까운 데이터 포인트 간의 관계를 최대한 유지**하며, 저차원에서도 로컬 군집 유지됨.
        - **글로벌 구조 표현에 약함** : **멀리 떨어진 데이터 포인트 간의 관계는 왜곡될 가능성** 있음
        - **계산 비용이 높음** : 고차원 데이터가 크면 계산량이 많고 학습 시간이 오래 걸림
        - **해석이 어려움** : 결과로 나온 저차원 데이터가 원래 데이터의 실제 분포를 완벽히 반영하지 않음.
3. 정리

![Image](https://github.com/user-attachments/assets/273fc733-bb7c-4daa-8ac1-614193ee9242)

---

# 모델 성능에 영향을 미치는 주요 Feature

## 모델 해석의 중요성

![Image](https://github.com/user-attachments/assets/0b54b0b1-a79c-48c6-8103-d4b3e2a9f413)

![Image](https://github.com/user-attachments/assets/715d5cbc-ad62-4d64-9f1b-1c49969f9a52)

## Feature 선택 및 중요도 평가 - Feature Importance (특성 중요도)

### 특징

- 모델이 학습하는 과정에서 **각 Feature가 예측에 얼마나 기여했는지**를 평가함
- 주로 **트리 기반 모델** (예: `RandomForest`, `XGBoost`)에 주로 사용된다.
- 각 Feature가 노드 분할에 얼마나 자주 사용되었는지, 분할로 인해 감소된 불순도 (Gini Impurity, Entropy 등) 기준으로 계산
- **Feature의 중요도를 상대적인 값**으로 제공한다.

### 장점

- **빠른 계산** : 모델 학습 과정에서 Feature Importance를 자동으로 계산
- **직관적 해석** : 중요도가 높은 Feature를 바로 식별

### 단점

- **Feature 상관성 문제** : 서로 상관관계가 높은 Feature가 있을 경우, **중요도가 왜곡**될 수 있음. (예: **중복된 Feature가 높은 중요도를 나누어 가지는 경우**)
- **특정 모델에 의존** : 선형 모델이나 트리 기반 모델이 아닌 경우, 직접 계산이 불가하다.

## Feature 선택 및 중요도 평가 - Drop-Column Importance

### 특징

- **각 Feature를 하나씩 제거**하고 **모델 성능에 미치는 영향을 측정**하여 **중요도를 평가**하는 방식
- 모델 학습 및 예측에서 해당 Feature가 기여하는 정도를 직접적으로 확인하기 때문에 매우 직관적이고 정확한 방법이다.

### 장점

- **모델의 독립성** : 모델의 **유형에 상관없이** 적용 가능
- **정확한 중요도 계산** : 각 Feature의 정확도를 직접 측정하기 때문에 **가장 신뢰도가 높은 평가 방법**

### 단점

- **높은 계산 비용** : 각 Feature 마다 모델을 재학습 해야 하기 때문에, Feature 개수가 많으면 모델학습 시간이 오래 걸린다.
- **데이터 의존성** : 데이터 세트가 작은 경우, Feature를 제거할 때, 성능 변화가 과대 또는 과소 평가 될 가능성이 있다.

## Feature 선택 및 중요도 평가 - Permutation Importance

### 특징

- 모델 학습 후 **Feature 값을 섞어 (permute) 모델 성능이 얼마나 저하되는지를 측정**하여 각 Feature의 중요도를 평가하는 방법
- 한 번에 한 **Feature의 값을 랜덤하게 섞어** 모델 예측 성능의 변화를 측정한다.
- **성능 변화가 클수록 해당 Feature의 중요도가 높다**고 평가한다.

### 장점

- 모델 독립성, 쉽고 직관적임
- **상관성 문제 완화** : 상관성이 없는 Feature들에 대해 **독립적으로 중요도를 평가**하기 때문에, 일부 상관성 문제를 해결한다.

### 단점

- **계산 비용** : 모델을 여러 번 재평가 해야 하기 때문에, 데이터와 모델 크기에 따라 시간이 **오래 걸릴 수 있다**.
- **랜덤성** : 데이터가 섞기가 랜덤이기 때문에, **중요도가 약간 변동**될 수 있다. (해결법 : 여러 번 반복 후 평균을 계산한다.)

## Feature 선택 및 중요도 평가 - SHAP

### 특징

- 기계학습 모델의 예측을 설명하기 위한 **해석 가능한 인공지능 기법**
- 게임 이론의 **Shapley 값을 기반**으로 **각 Feature 가 모델의 예측에 미친 영향을 정량적으로 평가**함
- 특정 Feature를 포함했을 때와 포함하지 않았을 때의 모델 출력 변화를 계산하여 기여도를 평가함.
- 모든 Feature 조합에 대해 반복적으로 평가하며, 결과적으로 Feature의 평균적 기여도를 계산

### 장점

- **모델 불가지론** : 어떤 모델에도 적용 가능함.
- **정확한 기여도 계산** : Feature의 상호작용을 고려하여 공정하게 계산
- **글로벌 해석** : 모델 전체에서 어떤 Feature가 중요한지 확인
- **로컬 해석** : 특정 예측값에 대해 각 Feature가 얼마다 기여했는지 석

### 단점

- **계산 비용** : Shapley 값 계산은 조합을 반복적으로 평가해야 하기 때문에 계산 비용이 높다.
- **데이터 스케일링** : 모델에 따라 Feature 스케일링이 필요한 경우, SHAP 값 해석 전에 이를 고려해야 한다.
- **Feature 상호작용** : SHAP은 Feature 간의 상호작용을 분석할 수 있지만, 해석이 복잡할 수 있다.
