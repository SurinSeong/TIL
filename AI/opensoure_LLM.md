# 오픈소스 LLM

## 정의

- 대규모 언어 모델의 소스코드와 가중치가 공개되어 있어 **누구나 접근하고 수정할 수 있는** 인공지능 모델
- 방대한 텍스트 데이터를 학습하여 **인간과 유사한 언어 이해 및 생성 능력**을 갖추고 있음

## 특징

- **자유로운 수정 및 맞춤화**
    - 소스 코드와 모델 가중치가 공개되어 있어 연구자나 개발자가 모델을 분석하고, 품질을 평가하며, 맞춤형 변형을 구축할 수 있음.
    - 특정 용도나 도메인에 최적화된 모델을 개발할 수 있음.
- **비용 효율성**
    - 오픈소스 LLM은 라이선스 비용없이 무료로 제공되기 때문에, 개인이나 기업이 추가적인 비용 부담없이 모델을 활용하고 자체 인프라에 배포할 수 있음.
- **투명성 및 커뮤니티 지원**
    - 모델의 내부 구조와 학습 방법이 공개되어 있어 투명성이 높고, 활발한 커뮤니티의 지원을 통해 지속적인 개선과 업데이트가 이루어짐.

## 파라미터 수에 따른 분류

### 1. 소형 모델 (10억 개 이하의 파라미터)

- **GPT-J (6B)** : EltutherAI에서 개발한 60억 개의 파라미터를 가진 모델로, GPT-3와 유사한 아키텍처를 갖추고 있음. 다양한 자연어 처리 작업에서 우수한 성능을 보이고, 오픈소스로 공개되어 연구 및 개발에 활용되고 있음.
- **Mistral (7B)** : 2023년 9월 27일에 출시된 모델로, 70억 개의 파라미터를 가지고 있음. 경량화된 구조로 인해 효율성이 높고, 다양한 자연어 처리 작업에서 우수한 성능을 발휘함.

### 2. 중형 모델 (10억 ~ 100억 개의 파라미터)

- **LLaMA 3 (8B)** : Meta AI에서 2024년 4월에 공개한 모델로, 80억 개의 파라미터를 가지고 있음. 이전 버전인 LLaMA 2에 비해 향상된 성능을 제공하고, 추론 및 코딩 능력이 강화되었음.
- **Jamba (52B)** : AI21 Labs에서 개발한 520억 개의 파라미터를 가진 모델로, Mamba 기반의 새로운 상태 공간 모델 (SSM)과 트랜스포너 하이브리드 아키텍처를 활용함. 혼합 전문가 (MoE) 기법을 사용하여 효율성과 성능을 모두 잡았으며, 최대 256K 토큰의 컨텍스트 윈도우를 지원함.

### 3. 대형 모델 (100억 개 이상의 파라미터)

- **Falcon 180B** : 기술 혁신 연구소에서 개발하여 2023년 9월 6일에 출시된 모델로, 1,800억 개의 파라미터를 지원함. 번역, 텍스트 생성, 연구와 같은 작업에 탁월한 성능을 보임.
- **DBRX (132B)** : Mosaic ML과 Databricks 팀이 공동 개발하여 2024년 3월 27일에 공개한 모델로, 1,320억 개의 파라미터를 가지고 있음. 혼합 전문가 (MoE) 트랜스포머 모델로, 토큰 당 360억 개의 파라미터가 활성화됨. 다양한 벤치마크에서 우수한 성능을 보이고, 연구 및 상업적 용도로 활용됨.
- DeepSeek V3 (671B) : MoE 기법을 활용하여 총 6,710억 개의 파라미터를 가진 모델로, 실제 연산에는 토큰 당 370억 개의 파라미터만 활성화됨. 이는 효율성과 성능을 동시에 추가한 설계임.
- LLaMA 3.1 (405B) : Meta AI에서 2024년 4월에 공개한 모델. 일반 지식 처리, 통제 가능성, 수학, 도구 사용, 다국어 번역 등에 있어 최상급 AI 모델임.

※ MoE (Mixture-of-Experts, 전문가 혼합)

## 공개 LLM을 사용하는 것이 효율적인 이유

### 1. 비용 절감

- ChatGPT와 같은 상용 LLM 모델을 사용할 경우, OpenAI API 또는 유사한 서비스의 **호출 비용이 지속적으로 발생**함. 특히, 트래픽이 많거나 기업 내부적으로 대규모 데이터를 처리하야할 경우, 비용이 상당히 증가할 수 있음.
- 반면, 오픈소스 LLM을 직접 호스팅하면 초기 인프라 구축 비용이 들지만, 장기적으로 운영비를 절감할 수 있음. 특히, Llama 2, Mistral, Falcon과 같은 모델을 **로컬 서버나 클라우드에서 실행하면 API 호출 당 요금 부담없이 지속적으로 사용**할 수 있음.

### 2. 데이터 프라이버시와 보안

- 상용 API 기반의 LLM은 입력 데이터를 외부 서버로 전송해야 하기 때문에, 기업의 내부 기밀이나 사용자 데이터 보호가 중요한 경우 **보안 리스크가 발생**할 수 있음.
- 반면, 공개 LLM을 자체 구축하면 **데이터가 외부로 유출되지 않고, 로컬 네트워크 또는 프라이빗 클라우드에서 안전하게 처리**됨. 금융, 의료, 법률과 같은 민감한 데이터를 다루는 기업이나 기관에서는 데이터 주권을 확보할 수 있다는 것이 큰 장점임.

### 3. 커스터마이징과 최적화 기능

- ChatGPT는 OpenAI가 제공하는 모델로, 특정 도메인에 특화된 튜닝이 어려움. 반면, 공개 LLM 모델은 사용자가 **직접 파인튜닝**하거나, **RAG 기법을 활용해 특정 데이터셋을 학습**시킬 수 있음.
- 예를 들어, 법률 자문 AI를 구축하고 싶다면, **오픈 소스 모델을 로컬 법률 데이터로 학습시켜 특정 법률 용어에 최적화할 수 있음**. 반면, ChatGPT API를 사용하면 프롬프트 엔지니어링 외에는 깊이 있는 커스터마이징이 어려움.
- 또한, **경량화된 모델**을 활용해서 **GPU 성능에 맞게** 최적화할 수 있으며, **Quantization(양자화)**를 적용해 성능과 메모리 사용을 최적화할 수도 있음.

### 4. 자율적인 운영 및 지속적인 개선 가능

- ChatGPT와 같은 SaaS 모델을 사용하면 OpenAI의 서비스 정책 변경, 가격 인상, API 제한 등의 영향을 받게 됨. 반면, 공개 LLM을 사용하면 특정 버전에 종속되지 않고, **모델을 지속적으로 업그레이드**하면서 **원하는 방식으로 운영**할 수 있음.
- 특히, **Open WebUI**와 같은 인터페이스를 활용하면 ChatGPT와 유사한 UX/UI 환경을 구축할 수 있고, 내부적으로 **워크플로우에 맞게 UI를 수정하고 기능을 추가**할 수 있음.

### 5. 오프라인 및 엣지 환경에서 사용 가능

- ChatGPT API는 인터넷 연결이 필수적이고, 네트워크 연결이 불안정하거나 외부 접속이 제한된 환경에서는 사용할 수 없음. 하지만, **공개 LLM 모델을 구축하면 오프라인 환경에서도 실행할 수 있기 때문**에 국방, 항공, 원격지 연구소 등의 분야에서도 활용할 수 있음.

### 6. 멀티모달 확장 가능성

- 최근 오픈소스 생태계에서는 텍스트 뿐만 아니라 음성, 이미지, 코드 생성 등의 멀티 모달 AI 모델이 빠르게 발전하고 있음. 이를 활용하면, 자체적으로 **음성 비서, 코드 자동 완성, 이미지 기반 질의응답 시스템**울 개발할 수 있음.
- 예를 들어, Stable Diffusion 같은 오픈소스 이미지 생성 모델과 통합하면 AI 기반 콘텐츠 생성 플랫폼을 만들 수 있고, **Whisper 같은 오픈소스 음성 인식 모델과 결합하면 음성 기반 AI 서비스**도 구축할 수 있음.
